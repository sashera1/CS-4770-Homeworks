{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sashera1/CS-4770-Homeworks/blob/main/CS_4770_NLP_Fall_2025_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ed91a23",
      "metadata": {
        "id": "2ed91a23"
      },
      "source": [
        "## Assignment 2: N-gram Language Models\n",
        "\n",
        "### *CS 4770 Natural Language Processing (Fall 2025)*\n",
        "\n",
        "In this assignment, you will implement N-gram language models (Unigram, Bigram, and Trigram) using Python. You will train your models on a given corpus, apply smoothing techniques, and evaluate your models using perplexity on a test set.\n",
        "\n",
        "### Tasks\n",
        "1. **Python implementation of N-gram Language Models**: Train Unigram, Bigram, and Trigram models.\n",
        "2. **Smoothing Techniques**: Implement add-k smoothing and interpolation.\n",
        "3. **Language Model Evaluation**: Using n-gram models to generate texts and calculate perplexity to evaluate your models.\n",
        "\n",
        "### Note\n",
        "Remember to delete ```raise NotImplementedError``` before completing the TODOs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Setup\n",
        "\n",
        "Run this cell to setup necessary packages and environments. No need to modify anything in this cell."
      ],
      "metadata": {
        "id": "5tShS--7dPH2"
      },
      "id": "5tShS--7dPH2"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1a5fcc59",
      "metadata": {
        "id": "1a5fcc59",
        "outputId": "77275e51-9db6-4939-cebf-06741319c15b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Import required packages\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Download the Reuters corpus\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86f4b7a4",
      "metadata": {
        "id": "86f4b7a4"
      },
      "source": [
        "### 1. Python implementation of N-gram Language Models\n",
        "\n",
        "Import the previously downloaded corpus, split it into training and test sets, and apply necessary preprocessing. No need to modify anything in this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "010b4860",
      "metadata": {
        "id": "010b4860"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown, reuters, inaugural\n",
        "\n",
        "# Load the Reuters corpus and split into training and test sets\n",
        "corpus = brown.sents()[:2000]\n",
        "\n",
        "# Add a start and end token for each sentence\n",
        "corpus = [ ['[BOS]'] + [w.lower() for w in sent] + ['[EOS]'] for sent in corpus]\n",
        "\n",
        "train_corpus, test_corpus = train_test_split(corpus, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f09d6454",
      "metadata": {
        "id": "f09d6454"
      },
      "source": [
        "#### 1.1 Unigram Language Model\n",
        "Calculate unigram probabilities based on the training set by completing the TODO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "64074811",
      "metadata": {
        "id": "64074811"
      },
      "outputs": [],
      "source": [
        "def train_unigram_lm(corpus):\n",
        "    unigram_counts = {}\n",
        "    unigram_model = {}\n",
        "\n",
        "    for sentence in corpus:\n",
        "        for word in sentence:\n",
        "            if word not in unigram_counts:\n",
        "                unigram_counts[word] = 1\n",
        "            else:\n",
        "                unigram_counts[word] += 1\n",
        "\n",
        "    total_words = sum(unigram_counts.values())\n",
        "\n",
        "    # TODO: Calculate unigram probabilities using unigram counts\n",
        "    for word in unigram_counts:\n",
        "      unigram_model[word] = unigram_counts[word]/total_words\n",
        "\n",
        "    return unigram_counts, unigram_model\n",
        "\n",
        "unigram_counts, unigram_model = train_unigram_lm(train_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b5ecfb4",
      "metadata": {
        "id": "1b5ecfb4"
      },
      "source": [
        "#### 1.2 Bigram Language Model\n",
        "Calculate bigram probabilities based on the training set by completing the TODO.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1c0ed704",
      "metadata": {
        "id": "1c0ed704"
      },
      "outputs": [],
      "source": [
        "def train_bigram_lm(corpus):\n",
        "    unigram_counts, _ = train_unigram_lm(corpus)\n",
        "    bigram_counts = {}\n",
        "    bigram_model = {}\n",
        "\n",
        "    for sentence in corpus:\n",
        "        for i in range(len(sentence) - 1):\n",
        "            bigram = (sentence[i], sentence[i + 1])\n",
        "            if bigram not in bigram_counts:\n",
        "                bigram_counts[bigram] = 1\n",
        "            else:\n",
        "                bigram_counts[bigram] += 1\n",
        "\n",
        "    # TODO: Calculate bigram probabilities using bigram counts\n",
        "    for bigram in bigram_counts:\n",
        "      bigram_model[bigram] = bigram_counts[bigram]/unigram_counts[bigram[0]]\n",
        "\n",
        "    return bigram_counts, bigram_model\n",
        "\n",
        "bigram_counts, bigram_model = train_bigram_lm(train_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68d0ab70",
      "metadata": {
        "id": "68d0ab70"
      },
      "source": [
        "#### 1.3 Trigram Language Model\n",
        "Calculate trigram probabilities based on the training set by completing the TODO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0b8ec576",
      "metadata": {
        "id": "0b8ec576"
      },
      "outputs": [],
      "source": [
        "def train_trigram_lm(corpus):\n",
        "    bigram_counts, _ = train_bigram_lm(corpus)\n",
        "    trigram_counts = {}\n",
        "    trigram_model = {}\n",
        "\n",
        "    for sentence in corpus:\n",
        "        for i in range(len(sentence) - 2):\n",
        "            trigram = (sentence[i], sentence[i + 1], sentence[i + 2])\n",
        "            if trigram not in trigram_counts:\n",
        "                trigram_counts[trigram] = 1\n",
        "            else:\n",
        "                trigram_counts[trigram] += 1\n",
        "\n",
        "    # TODO: Compute trigram probabilities using trigram counts\n",
        "    for trigram in trigram_counts:\n",
        "      trigram_model[trigram] = trigram_counts[trigram]/bigram_counts[(trigram[0],trigram[1])]\n",
        "\n",
        "    return trigram_counts, trigram_model\n",
        "\n",
        "trigram_counts, trigram_model = train_trigram_lm(train_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cef7ee8",
      "metadata": {
        "id": "9cef7ee8"
      },
      "source": [
        "#### 1.4 Using N-gram Language Models\n",
        "\n",
        "Just run this cell and copy the printed output to your latex. No need to modify anything in this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d7611da0",
      "metadata": {
        "id": "d7611da0",
        "outputId": "3e6ee3e3-bdc7-4275-898f-6ffd189d73f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample sentence: ['[BOS]', 'five', 'candidates', 'seek', 'the', 'place', 'vacated', 'by', 'secretary', 'hugh', 'g.', 'stout', '.', '[EOS]']\n",
            "Unigram probability: 4.122943312604837e-42\n",
            "Bigram probability: 1.3991150205742658e-14\n",
            "Trigram probability: 0.00015624999999999998\n"
          ]
        }
      ],
      "source": [
        "# Calculate the probability of a sample sentence using the Unigram, Bigram, and Trigram models\n",
        "\n",
        "def calulate_unigram_prob(sentence, unigram_model):\n",
        "    for idx in range(len(sentence)):\n",
        "        if idx == 0:\n",
        "            prob = 1 # assume the sentence always starts with [BOS]\n",
        "        else:\n",
        "            prob *= unigram_model.get(sentence[idx], 0)\n",
        "    return prob\n",
        "\n",
        "\n",
        "def calculate_bigram_prob(sentence, bigram_model):\n",
        "    for idx in range(len(sentence)):\n",
        "        if idx == 0:\n",
        "            prob = 1 # assume the sentence always starts with [BOS]\n",
        "        else:\n",
        "            prob *= bigram_model.get((sentence[idx-1], sentence[idx]), 0)\n",
        "    return prob\n",
        "\n",
        "def calculate_trigram_prob(sentence, trigram_model, bigram_model):\n",
        "    for idx in range(len(sentence)):\n",
        "        if idx == 0:\n",
        "            prob = 1 # assume the sentence always starts with [BOS]\n",
        "        elif idx == 1:\n",
        "            prob *= bigram_model.get((sentence[0], sentence[1]), 0)\n",
        "        else:\n",
        "            prob *= trigram_model.get((sentence[idx-2], sentence[idx-1], sentence[idx]), 0)\n",
        "    return prob\n",
        "\n",
        "sample_sentence = train_corpus[0]\n",
        "print(f'Sample sentence: {sample_sentence}')\n",
        "\n",
        "# Test the Unigram, Bigram, and Trigram models\n",
        "unigram_prob = calulate_unigram_prob(sample_sentence, unigram_model)\n",
        "bigram_prob = calculate_bigram_prob(sample_sentence, bigram_model)\n",
        "trigram_prob = calculate_trigram_prob(sample_sentence, trigram_model, bigram_model)\n",
        "\n",
        "print(f\"Unigram probability: {unigram_prob}\")\n",
        "print(f\"Bigram probability: {bigram_prob}\")\n",
        "print(f\"Trigram probability: {trigram_prob}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c9c963",
      "metadata": {
        "id": "e7c9c963"
      },
      "source": [
        "### 2. Python Implementation of Smoothing\n",
        "\n",
        "Implement add-k smoothing and interpolation for your N-gram models by completing the TODO. Compare the differences in probabilities before and after applying smoothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a21bba37",
      "metadata": {
        "id": "a21bba37",
        "outputId": "f13a29ac-1995-4358-ce55-654e07384462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_sentence: ['[BOS]', \"you'll\", 'probably', 'get', 'a', 'ball', 'bat', 'on', 'the', 'head', '.', '[EOS]']\n",
            "Bigram probabilities (before smoothing): 0.0\n",
            "Smoothed Bigram probabilities (add-k): 1.6706506325434276e-35\n"
          ]
        }
      ],
      "source": [
        "def add_k_smoothing(unigram_counts, bigram_counts, k):\n",
        "    smoothed_bigram_model = {}\n",
        "\n",
        "    for word_1 in unigram_counts:\n",
        "        for word_2 in unigram_counts:\n",
        "            #will be called like this: .get(word before, word, k) let word_1 be word_before, word_2 be word\n",
        "            #p(word|word before) > p(word_2, word_1)\n",
        "            # #(word before, word)+k / #(word before)+k|V| > #(word_1,word_2)+k / #(word_1)+k|V|\n",
        "            if (word_1,word_2) in bigram_counts:\n",
        "              numerator_sequence = bigram_counts[(word_1,word_2)]\n",
        "            else:\n",
        "              numerator_sequence = 0\n",
        "            numerator = numerator_sequence + k\n",
        "            denominator = unigram_counts[word_1] + k*len(unigram_counts)\n",
        "            smoothed_probability = numerator/denominator\n",
        "            smoothed_bigram_model[(word_1,word_2)] = smoothed_probability\n",
        "\n",
        "    return smoothed_bigram_model\n",
        "\n",
        "smoothed_bigram_model = add_k_smoothing(unigram_counts, bigram_counts, 0.4)\n",
        "\n",
        "sample_sentence = test_corpus[5]\n",
        "\n",
        "print(f'sample_sentence: {sample_sentence}')\n",
        "print(f'Bigram probabilities (before smoothing): {calculate_bigram_prob(sample_sentence, bigram_model)}')\n",
        "print(f\"Smoothed Bigram probabilities (add-k): {calculate_bigram_prob(sample_sentence, smoothed_bigram_model)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3fdf524e",
      "metadata": {
        "id": "3fdf524e",
        "outputId": "99f8d737-c2ec-4802-e905-071904bfc489",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram probabilities (before interpolation): 0.0\n",
            "Interpolated Trigram probabilities: 3.0825212308551035e-08\n"
          ]
        }
      ],
      "source": [
        "def interpolate_trigram_prob(sentence, trigram_model, bigram_model, unigram_model, lambdas):\n",
        "    for idx in range(len(sentence)):\n",
        "        if idx == 0:\n",
        "            prob = 1 # assume the sentence always starts with [BOS]\n",
        "        elif idx == 1:\n",
        "            prob *= bigram_model.get((sentence[0], sentence[1]), 0)\n",
        "        else:\n",
        "            # TODO: Compute interpolated trigram probabilities\n",
        "            # prob *= p_interpolate(x_i | x_i-2, x_i-2)\n",
        "            # = l_1*p(x_i) + l_2*p(x_i | x_i-1) + 1_3*p(x_i | x_i-1,x_i-2)\n",
        "\n",
        "\n",
        "            unigram_term = lambdas[0]*unigram_model[sentence[idx]]\n",
        "            bigram_term = lambdas[1]*bigram_model.get((sentence[idx-1],sentence[idx]),0.0)\n",
        "            trigram_term = lambdas[2]*trigram_model.get((sentence[idx-2],sentence[idx-1],sentence[idx]),0.0)\n",
        "            p_interpolate = unigram_term + bigram_term + trigram_term\n",
        "            prob *= p_interpolate\n",
        "\n",
        "    return prob\n",
        "\n",
        "sample_sentence = test_corpus[8]\n",
        "\n",
        "print(f'Trigram probabilities (before interpolation): {calculate_trigram_prob(sample_sentence, trigram_model, bigram_model)}')\n",
        "print(f'Interpolated Trigram probabilities: {interpolate_trigram_prob(sample_sentence, trigram_model, bigram_model, unigram_model, [0.1, 0.3, 0.6])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc293bad",
      "metadata": {
        "id": "fc293bad"
      },
      "source": [
        "\n",
        "### 3. N-gram Language Model Evaluation\n",
        "\n",
        "Evaluate your trained N-gram models with perplexity on the test set by completing the TODO. Answers to both TODOs are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "25313658",
      "metadata": {
        "id": "25313658",
        "outputId": "58b366fb-6278-473b-f3aa-15f780ebee65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoothed Bigram Perplexity (add-k): 466002.42391135567\n"
          ]
        }
      ],
      "source": [
        "def calculate_bigram_perplexity(sentence, smoothed_bigram_model):\n",
        "    \"\"\"\n",
        "    Calculates the perplexity of a sentence using the add-k smoothed bigram model.\n",
        "    \"\"\"\n",
        "    perplexity = 0\n",
        "    for idx in range(len(sentence)):\n",
        "        if idx == 0:\n",
        "            prob = 1  # assume the sentence always starts with [BOS]\n",
        "        else:\n",
        "            prob = smoothed_bigram_model.get((sentence[idx - 1], sentence[idx]), 1e-10)  # use a small prob value for unseen bigrams to avoid nan perplexity\n",
        "\n",
        "        perplexity += np.log(prob)\n",
        "\n",
        "    # TODO: Compute perplexity\n",
        "    perplexity *= (-1/len(sentence))\n",
        "    perplexity = np.exp(perplexity)\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "# Calculate PPL for add-k smoothed bigram model\n",
        "bigram_perplexity = []\n",
        "for sentence in test_corpus:\n",
        "    perplexity = calculate_bigram_perplexity(sentence, smoothed_bigram_model)\n",
        "    bigram_perplexity.append(perplexity)\n",
        "\n",
        "print(f\"Smoothed Bigram Perplexity (add-k): {np.mean(bigram_perplexity)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_trigram_perplexity(sentence, trigram_model, bigram_model, unigram_model, lambdas):\n",
        "    \"\"\"\n",
        "    Calculates the perplexity of a sentence using the interpolated trigram model.\n",
        "    \"\"\"\n",
        "    perplexity = 0\n",
        "    for idx in range(len(sentence)):\n",
        "        if idx == 0:\n",
        "            prob = 1  # assume the sentence always starts with [BOS]\n",
        "        elif idx == 1:\n",
        "            prob = bigram_model.get((sentence[0], sentence[1]), 1e-10)  # use a small prob value for unseen bigrams to avoid nan perplexity\n",
        "        else:\n",
        "            prob = lambdas[0] * unigram_model.get(sentence[idx], 1e-10) + lambdas[1] * bigram_model.get((sentence[idx-1], sentence[idx]), 1e-10) + lambdas[2] * trigram_model.get((sentence[idx-2], sentence[idx-1], sentence[idx]), 1e-10)\n",
        "\n",
        "        perplexity += np.log(prob)\n",
        "\n",
        "    # TODO: Compute perplexity\n",
        "    perplexity *= (-1/len(sentence))\n",
        "    perplexity = np.exp(perplexity)\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "# Calculate PPL for interpolated trigram model\n",
        "trigram_perplexity = []\n",
        "for sentence in test_corpus:\n",
        "    perplexity = calculate_trigram_perplexity(sentence, trigram_model, bigram_model, unigram_model, [0.1, 0.3, 0.6])\n",
        "    trigram_perplexity.append(perplexity)\n",
        "\n",
        "print(f\"Interpolated Trigram Perplexity: {np.mean(trigram_perplexity)}\")"
      ],
      "metadata": {
        "id": "pkP5ya-vrLS_",
        "outputId": "103fb12a-59b6-46d9-ae93-b6549d25d38d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pkP5ya-vrLS_",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interpolated Trigram Perplexity: 13402.71588398677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using your trained n-gram models to generate new texts with the greedy decoding startegy by completing the TODOs."
      ],
      "metadata": {
        "id": "8gZQDZUDtLKu"
      },
      "id": "8gZQDZUDtLKu"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_unigram(unigram_model, max_seq_len):\n",
        "    \"\"\"\n",
        "    Generates text using the unigram model with greedy decoding.\n",
        "    \"\"\"\n",
        "    current_token = \"[BOS]\"\n",
        "    generated_text = [current_token]\n",
        "    for _ in range(max_seq_len - 1):\n",
        "        max_prob = 0\n",
        "        for word in unigram_model:\n",
        "            unigram_prob = unigram_model[word]\n",
        "\n",
        "            # TODO: Select the next token using greedy decoding\n",
        "            if unigram_prob>max_prob:\n",
        "              max_prob=unigram_prob\n",
        "              next_token = word\n",
        "\n",
        "        generated_text.append(next_token)\n",
        "        current_token = next_token\n",
        "\n",
        "        if current_token == \"[EOS]\":\n",
        "            break\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate text using the unigram model\n",
        "generated_text = generate_text_unigram(unigram_model, 10)\n",
        "print(f\"Unigram generated texts: {' '.join(generated_text)}\")"
      ],
      "metadata": {
        "id": "HS5sRydm0YCX",
        "outputId": "db102c98-da39-428c-f079-9620214c2fd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HS5sRydm0YCX",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram generated texts: [BOS] the the the the the the the the the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_bigram(bigram_model, unigram_model, max_seq_len):\n",
        "    \"\"\"\n",
        "    Generates text using the bigram model with greedy decoding.\n",
        "    \"\"\"\n",
        "    current_token = \"[BOS]\"\n",
        "    generated_text = [current_token]\n",
        "    for _ in range(max_seq_len - 1):\n",
        "        max_prob = 0\n",
        "        for word in unigram_model:\n",
        "            bigram_prob = bigram_model.get((current_token, word), 0)\n",
        "\n",
        "            # TODO: Select the next token using greedy decoding\n",
        "            if bigram_prob > max_prob:\n",
        "              max_prob = bigram_prob\n",
        "              next_token = word\n",
        "\n",
        "        generated_text.append(next_token)\n",
        "        current_token = next_token\n",
        "\n",
        "        if current_token == \"[EOS]\":\n",
        "            break\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate text using the bigram model\n",
        "generated_text = generate_text_bigram(bigram_model, unigram_model, 10)\n",
        "print(f\"Bigram generated texts: {' '.join(generated_text)}\")\n"
      ],
      "metadata": {
        "id": "JNMDdgAGoH8Z",
        "outputId": "22a775dd-a737-4cce-f050-09b5246b9671",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JNMDdgAGoH8Z",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram generated texts: [BOS] the state college , the state college , the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_trigram(trigram_model, bigram_model, unigram_model, max_seq_len):\n",
        "    \"\"\"\n",
        "    Generates text using the trigram model with greedy decoding.\n",
        "    \"\"\"\n",
        "    current_token_1 = \"[BOS]\"\n",
        "    current_token_2 = None\n",
        "    generated_text = [current_token_1]\n",
        "\n",
        "    # Use bigram model to generate the second token\n",
        "    max_prob = 0\n",
        "    for word in unigram_model:\n",
        "        # TODO: Select the second token using greedy decoding\n",
        "        bigram_prob = bigram_model.get((current_token_1, word), 0)\n",
        "\n",
        "        if bigram_prob > max_prob:\n",
        "          max_prob = bigram_prob\n",
        "          next_token = word\n",
        "\n",
        "    generated_text.append(next_token)\n",
        "    current_token_2 = next_token\n",
        "\n",
        "    if current_token_2 == \"[EOS]\":\n",
        "        return generated_text\n",
        "\n",
        "    # Use trigram model to generate the remaining tokens\n",
        "\n",
        "    for _ in range(max_seq_len - 2):\n",
        "        max_prob = 0\n",
        "        for word in unigram_model:\n",
        "\n",
        "            # TODO: Select the next token using greedy decoding\n",
        "          trigram_prob = trigram_model.get((current_token_1, current_token_2, word), 0)\n",
        "          if trigram_prob > max_prob:\n",
        "            max_prob = trigram_prob\n",
        "            next_token = word\n",
        "\n",
        "        generated_text.append(next_token)\n",
        "        current_token_1 = current_token_2\n",
        "        current_token_2 = next_token\n",
        "\n",
        "        if current_token_2 == \"[EOS]\":\n",
        "            break\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate text using the trigram model\n",
        "generated_text = generate_text_trigram(trigram_model, bigram_model, unigram_model, 15)\n",
        "print(f\"Trigram generated texts: {' '.join(generated_text)}\")\n"
      ],
      "metadata": {
        "id": "DSrL-23oQ5ma",
        "outputId": "8df46567-ba81-472b-f8a9-6cb285b5cc5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DSrL-23oQ5ma",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram generated texts: [BOS] the president said he was the first year in the early stages , garden\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}